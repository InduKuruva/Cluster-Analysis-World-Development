# -*- coding: utf-8 -*-
"""Grid_Model_Deployment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fFM-CLwaqM2O8KolrD5J8e_c9SnNx0U2
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile streamlit_app.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# import joblib
# from sklearn.preprocessing import StandardScaler
# from sklearn.decomposition import PCA
# import seaborn as sns
# import matplotlib.pyplot as plt
# import io
# 
# # --- Load pre-trained models and parameters ---
# @st.cache_resource
# def load_model_components():
#     scaler_filename = 'standard_scaler.joblib'
#     preprocessing_params_filename = 'grid_preprocessing_params.joblib'
# 
#     scaler = None  # Initialize scaler
#     params = None  # Initialize params
# 
#     try:
#         scaler = joblib.load(scaler_filename)
#         params = joblib.load(preprocessing_params_filename)
#     except FileNotFoundError:
#         st.error("Model files not found. Please ensure 'standard_scaler.joblib' and 'grid_preprocessing_params.joblib' are in the same directory.")
#         st.stop() # This will stop the Streamlit app immediately if files are missing
#     return scaler, params
# 
# scaler, loaded_params = load_model_components()
# 
# # Initialize parameters to default empty values for safety
# columns_to_drop = []
# currency_columns = []
# median_values = {}
# numerical_cols_to_scale = []
# categorical_cols_to_ohe = []
# final_feature_columns_order = []
# bin_edges = {}
# features_for_grid = []
# cluster_map = {}
# 
# # Only proceed with extracting parameters if model components were loaded successfully
# # This check is crucial for handling st.stop() behavior in non-Streamlit environments
# if scaler is not None and loaded_params is not None:
#     columns_to_drop = loaded_params['columns_to_drop']
#     currency_columns = loaded_params['currency_columns']
#     median_values = loaded_params['median_values']
#     numerical_cols_to_scale = loaded_params['numerical_cols_to_scale']
#     categorical_cols_to_ohe = loaded_params['categorical_cols_to_ohe']
#     final_feature_columns_order = loaded_params['final_feature_columns_order']
#     bin_edges = loaded_params['bin_edges']
#     features_for_grid = loaded_params['features_for_grid']
#     cluster_map = loaded_params['cluster_map']
# else:
#     # If loading failed (and st.stop() did not exit the script entirely, e.g., in Colab),
#     # ensure that the app effectively stops here.
#     st.error("Application could not load necessary model components. Exiting.")
#     st.stop()
# 
# # --- Helper Functions (as used during training and deployment) ---
# def clean_currency_for_deployment(series):
#     if series.dtype == 'object':
#         # Ensure the replace method handles potential NaN values before string operations
#         return series.astype(str).str.replace('$', '', regex=False).str.replace(',', '', regex=False).astype(float, errors='ignore')
#     return series
# 
# def get_cell_coordinates_for_deployment(row, bin_edges_data, features_data):
#     coords = []
#     for feature in features_data:
#         # Handle cases where feature might not be present in the row, or value is NaN
#         if feature in row and pd.notna(row[feature]):
#             coords.append(np.digitize(row[feature], bin_edges_data[feature]) - 1) # -1 to make it 0-indexed
#         else:
#             # Assign a default coordinate (-1) to indicate unbinned or missing value for this feature
#             coords.append(-1)
#     return tuple(coords)
# 
# @st.cache_data
# def process_and_cluster_data(new_data_df):
#     # Make a copy to avoid modifying the original uploaded DataFrame
#     processed_new_data = new_data_df.copy()
#     original_new_data_for_merge = new_data_df.copy() # To attach clusters to original data structure
# 
#     # 1. Drop columns identified during training
#     processed_new_data = processed_new_data.drop(columns=columns_to_drop, errors='ignore')
# 
#     # 2. Clean and convert currency columns
#     for col in currency_columns:
#         if col in processed_new_data.columns:
#             processed_new_data[col] = clean_currency_for_deployment(processed_new_data[col])
# 
#     # 3. Impute numerical columns with stored medians
#     for col in numerical_cols_to_scale:
#         if col in processed_new_data.columns and processed_new_data[col].isnull().any():
#             if col in median_values:
#                 processed_new_data[col].fillna(median_values[col], inplace=True)
#             else:
#                 # Fallback: if a numerical column had no NaNs in training but does now, fill with 0
#                 processed_new_data[col] = processed_new_data[col].fillna(0)
# 
#     # 4. Scale numerical features
#     # Ensure all numerical_cols_to_scale are present before scaling
#     # If a numerical column is missing from new_data, add it and fill with 0 before scaling
#     for col in numerical_cols_to_scale:
#         if col not in processed_new_data.columns:
#             processed_new_data[col] = 0.0
#     processed_new_data[numerical_cols_to_scale] = scaler.transform(processed_new_data[numerical_cols_to_scale])
# 
#     # 5. One-hot encode categorical features
#     if categorical_cols_to_ohe:
#         for col in categorical_cols_to_ohe:
#             if col not in processed_new_data.columns:
#                 # If a categorical column is missing in new data, add it with a placeholder 'Unknown'
#                 # This ensures the column exists before one-hot encoding.
#                 processed_new_data[col] = 'Unknown'
# 
#         # Perform one-hot encoding
#         processed_new_data = pd.get_dummies(processed_new_data, columns=categorical_cols_to_ohe, drop_first=True)
# 
#     # 6. Reindex to ensure feature consistency with training data
#     # Add missing columns (from training set but not in new data) and fill with 0
#     # Drop extra columns (in new data but not in training set)
#     input_for_prediction = processed_new_data.reindex(columns=final_feature_columns_order, fill_value=0)
# 
#     # --- Grid-based Clustering Prediction Logic ---
#     new_data_cell_coordinates = input_for_prediction.apply(
#         lambda row: get_cell_coordinates_for_deployment(row, bin_edges, features_for_grid), axis=1
#     )
# 
#     # Assign Grid_Cluster labels using the loaded cluster_map
#     # Default to -1 (noise) if a cell coordinate is not in the cluster_map
#     predictions = new_data_cell_coordinates.apply(lambda x: cluster_map.get(x, -1))
# 
#     # Add predictions to the original_new_data_for_merge DataFrame for context
#     original_new_data_for_merge['Grid_Cluster'] = predictions
# 
#     return original_new_data_for_merge, input_for_prediction # Also return input_for_prediction for PCA
# 
# 
# # --- Streamlit Application ---
# st.set_page_config(layout="wide", page_title="Grid-based Clustering App")
# 
# st.title("ðŸŒ World Development Measurement Clustering")
# st.write("Upload an Excel file to apply the pre-trained Grid-based Clustering model and visualize the results.")
# 
# uploaded_file = st.file_uploader("Choose an Excel file", type=["xlsx", "xls"])
# 
# if uploaded_file is not None:
#     try:
#         # Read the uploaded Excel file into a Pandas DataFrame
#         data_to_cluster = pd.read_excel(uploaded_file)
#         st.success("File uploaded successfully!")
#         st.subheader("Original Data Head")
#         st.dataframe(data_to_cluster.head())
# 
#         # Process data and apply clustering
#         clustered_data_with_original_cols, features_for_pca = process_and_cluster_data(data_to_cluster)
# 
#         st.subheader("Clustering Results")
#         st.write("Data with assigned Grid_Cluster labels:")
#         st.dataframe(clustered_data_with_original_cols.head())
#         st.write("Cluster Distribution:")
#         st.dataframe(clustered_data_with_original_cols['Grid_Cluster'].value_counts().reset_index().rename(columns={'index': 'Cluster', 'Grid_Cluster': 'Count'}))
# 
#         # --- PCA for Visualization ---
#         # Ensure we have enough samples and features for PCA
#         # Filter out rows where Grid_Cluster is -1 (noise) for better visualization, but keep original for other analysis
#         # PCA requires at least 2 features and more samples than features
#         if not features_for_pca.empty and features_for_pca.shape[1] >= 2 and features_for_pca.shape[0] > features_for_pca.shape[1]:
#             try:
#                 pca = PCA(n_components=2)
#                 pca_components = pca.fit_transform(features_for_pca)
#                 pca_result_df = pd.DataFrame(data=pca_components, columns=['PC1', 'PC2'])
#                 pca_result_df['Grid_Cluster'] = clustered_data_with_original_cols['Grid_Cluster']
# 
#                 st.subheader("PCA Visualization of Clusters")
#                 fig, ax = plt.subplots(figsize=(10, 8))
#                 sns.scatterplot(
#                     x='PC1',
#                     y='PC2',
#                     hue='Grid_Cluster',
#                     data=pca_result_df,
#                     palette='viridis',
#                     legend='full',
#                     ax=ax
#                 )
#                 ax.set_title('Grid-based Clustering Visualization with PCA')
#                 ax.set_xlabel('Principal Component 1')
#                 ax.set_ylabel('Principal Component 2')
#                 ax.grid(True)
#                 st.pyplot(fig)
#                 plt.close(fig) # Close the plot to free up memory
# 
#             except Exception as e:
#                 st.warning(f"Could not perform PCA visualization. Error: {e}")
#                 st.info("This might happen if the data is too sparse or has insufficient variance after preprocessing.")
#         else:
#             st.warning("Insufficient data or features after preprocessing to perform PCA visualization (need at least 2 features and more samples than features).")
# 
#         # --- Download Results ---
#         st.subheader("Download Results")
#         csv_buffer = io.StringIO()
#         clustered_data_with_original_cols.to_csv(csv_buffer, index=False)
#         st.download_button(
#             label="Download Clustered Data as CSV",
#             data=csv_buffer.getvalue(),
#             file_name="clustered_world_development_data.csv",
#             mime="text/csv",
#         )
# 
#     except Exception as e:
#         st.error(f"Error processing file: {e}")
#         st.write("Please ensure the uploaded file is a valid Excel format and its structure is similar to the training data.")
# else:
#     st.info("Please upload an Excel file to get started.")
#

