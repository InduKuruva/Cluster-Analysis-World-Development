# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aqexHDXolrYTYdaglFPMWxA--vQPyZjM
"""

import streamlit as st
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

st.set_page_config(page_title="World Development Clustering", layout="wide")

st.title("ðŸŒ World Development Clustering Application")

# Upload file
uploaded_file = st.file_uploader("Upload World Development Dataset (Excel)", type=["xlsx"])

if uploaded_file is not None:

    data = pd.read_excel(uploaded_file)
    st.subheader("Raw Dataset")
    st.dataframe(data.head())

    # -------------------------
    # DATA PREPROCESSING
    # -------------------------

    # Drop columns with >40% missing values
    missing_percentage = data.isnull().sum() / len(data) * 100
    columns_to_drop = missing_percentage[missing_percentage > 40].index.tolist()
    data = data.drop(columns=columns_to_drop)

    # Clean currency columns if present
    def clean_currency(series):
        if series.dtype == 'object':
            return series.astype(str).str.replace('$', '').str.replace(',', '', regex=False).astype(float, errors='ignore')
        return series

    for col in data.columns:
        data[col] = clean_currency(data[col])

    # Impute numerical columns
    for col in data.select_dtypes(include=['float64', 'int64']).columns:
        data[col].fillna(data[col].median(), inplace=True)

    # Remove duplicates
    data.drop_duplicates(inplace=True)

    # Separate features
    numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns.tolist()
    categorical_cols = data.select_dtypes(include=['object']).columns.tolist()

    # Scaling
    scaler = StandardScaler()
    data[numerical_cols] = scaler.fit_transform(data[numerical_cols])

    # One-hot encoding
    if categorical_cols:
        df = pd.get_dummies(data, columns=categorical_cols, drop_first=True)
    else:
        df = data.copy()

    st.success("Preprocessing Completed Successfully!")

    # -------------------------
    # MODEL SELECTION
    # -------------------------

    st.sidebar.header("Select Clustering Model")

    model_choice = st.sidebar.selectbox(
        "Choose Model",
        ["KMeans", "DBSCAN", "Hierarchical", "GMM"]
    )

    if model_choice == "KMeans":
        k = st.sidebar.slider("Select Number of Clusters (K)", 2, 10, 4)
        model = KMeans(n_clusters=k, random_state=42, n_init='auto')
        labels = model.fit_predict(df)

    elif model_choice == "DBSCAN":
        eps = st.sidebar.slider("Epsilon (eps)", 0.1, 2.0, 0.5)
        min_samples = st.sidebar.slider("Min Samples", 2, 10, 5)
        model = DBSCAN(eps=eps, min_samples=min_samples)
        labels = model.fit_predict(df)

    elif model_choice == "Hierarchical":
        k = st.sidebar.slider("Select Number of Clusters", 2, 10, 3)
        model = AgglomerativeClustering(n_clusters=k)
        labels = model.fit_predict(df)

    elif model_choice == "GMM":
        k = st.sidebar.slider("Select Number of Components", 2, 10, 4)
        model = GaussianMixture(n_components=k, random_state=42)
        labels = model.fit(df).predict(df)

    df["Cluster"] = labels

    st.subheader("Cluster Distribution")
    st.write(df["Cluster"].value_counts())

    # -------------------------
    # PCA VISUALIZATION
    # -------------------------

    pca = PCA(n_components=2)
    pca_components = pca.fit_transform(df.drop(columns=["Cluster"]))

    pca_df = pd.DataFrame(pca_components, columns=["PC1", "PC2"])
    pca_df["Cluster"] = labels

    fig, ax = plt.subplots(figsize=(8, 6))
    sns.scatterplot(data=pca_df, x="PC1", y="PC2", hue="Cluster", palette="viridis", ax=ax)
    ax.set_title(f"{model_choice} Clustering (PCA View)")
    st.pyplot(fig)

    # -------------------------
    # EVALUATION METRICS
    # -------------------------

    if model_choice != "DBSCAN" or len(set(labels)) > 1:
        if len(set(labels)) > 1 and -1 not in set(labels):
            silhouette = silhouette_score(df.drop(columns=["Cluster"]), labels)
            davies = davies_bouldin_score(df.drop(columns=["Cluster"]), labels)
            calinski = calinski_harabasz_score(df.drop(columns=["Cluster"]), labels)

            st.subheader("Model Evaluation")
            st.write(f"Silhouette Score: {silhouette:.3f}")
            st.write(f"Davies-Bouldin Index: {davies:.3f}")
            st.write(f"Calinski-Harabasz Index: {calinski:.3f}")
        else:
            st.warning("Evaluation metrics not valid (Noise or single cluster detected).")
    # -------------------------
    # COUNTRY DEVELOPMENT CLASSIFICATION
    # -------------------------

    st.subheader("ðŸŒ Country Development Classification")

    # Only proceed if Country column exists
    if "Country" in data.columns:

        # Add original country names back
        df_with_country = df.copy()
        df_with_country["Country"] = data["Country"].values

        cluster_profile = (
            df_with_country.select_dtypes(include=["float64", "int64"]).groupby(df_with_country["Cluster"]).mean())

        st.write("Cluster Profile (Averages of Indicators)")
        st.dataframe(cluster_profile)

        # Try to determine development status based on GDP
        if "GDP" in cluster_profile.columns:

            # Rank clusters by GDP
            cluster_ranking = cluster_profile["GDP"].sort_values(ascending=False)

            developed_cluster = cluster_ranking.index[0]
            developing_cluster = cluster_ranking.index[-1]

            if len(cluster_ranking) >= 3:
                emerging_cluster = cluster_ranking.index[1]
            else:
                emerging_cluster = None

            # Map clusters to labels
            development_map = {}

            development_map[developed_cluster] = "Developed"
            development_map[developing_cluster] = "Developing"

            if emerging_cluster is not None:
                development_map[emerging_cluster] = "Emerging"

            df_with_country["Development_Status"] = df_with_country["Cluster"].map(development_map)

            # Show summary table
            summary_table = df_with_country.groupby("Development_Status")["Country"].count().reset_index()
            summary_table.columns = ["Development Status", "Number of Countries"]

            st.subheader("ðŸ“Š Development Category Summary")
            st.dataframe(summary_table)


            # Show countries under each category
            for category in summary_table["Development Status"].dropna():
              st.subheader(f"{category} Countries")
              countries_list = (df_with_country[df_with_country["Development_Status"] == category]["Country"].drop_duplicates().sort_values().reset_index(drop=True))
              st.write(f"Total Countries: {len(countries_list)}")
              st.dataframe(countries_list)


        else:
            st.warning("GDP column not found. Cannot classify development levels.")
    else:
        st.warning("Country column not found in dataset.")